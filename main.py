import os
import json
import uvicorn
import requests
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
from fastapi.responses import StreamingResponse
from starlette.concurrency import run_in_threadpool
import tempfile
import shutil

from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Import RAG service
try:
    from rag_service import ingest_document, search_context, build_rag_prompt, delete_document_vectors, get_document_chunks
    RAG_ENABLED = True
    print("RAG service loaded successfully")
except ImportError as e:
    RAG_ENABLED = False
    print(f"RAG service not available: {e}")

app = FastAPI(title="LLM Research Assistant AI Service")

# CORS configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configuration - Official DeepSeek API
API_KEY = os.getenv("DEEPSEEK_API_KEY")
BASE_URL = "https://api.deepseek.com/v1/chat/completions"
MODEL_NAME = "deepseek-chat"

# Models
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    message: str
    history: Optional[List[ChatMessage]] = []
    model: str = MODEL_NAME
    use_rag: bool = True  # Enable RAG by default
    doc_id: Optional[str] = None  # Optional: limit search to specific document

class IngestRequest(BaseModel):
    file_path: str
    doc_id: Optional[str] = None

class AnalysisRequest(BaseModel):
    doc_id: str
    type: str = "summary"  # summary, key_points

class ComparisonRequest(BaseModel):
    doc_ids: List[str]
    aspects: Optional[List[str]] = None

class WriteRequest(BaseModel):
    text: str
    instruction: str  # polish, expand, continue, fix_grammar
    context: Optional[str] = None

@app.get("/")
def read_root():
    return {
        "status": "ok", 
        "service": "LLM Research Assistant AI Service (DeepSeek)",
        "rag_enabled": RAG_ENABLED
    }

@app.post("/api/v1/ingest")
async def ingest_endpoint(file: UploadFile = File(...), doc_id: Optional[str] = None):
    """
    Ingest a document into the vector database.
    Accepts file upload and processes it for RAG.
    """
    if not RAG_ENABLED:
        raise HTTPException(status_code=503, detail="RAG service is not available")
    
    print(f"=== Ingesting document: {file.filename} ===")
    
    # Save uploaded file to temp location
    try:
        suffix = os.path.splitext(file.filename)[1]
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
            shutil.copyfileobj(file.file, tmp)
            tmp_path = tmp.name
        
        # Process document
        result = await run_in_threadpool(ingest_document, tmp_path, doc_id or file.filename)
        
        # Cleanup temp file
        os.unlink(tmp_path)
        
        return result
    except Exception as e:
        print(f"Ingestion error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/ingest/path")
async def ingest_by_path(request: IngestRequest):
    """
    Ingest a document by file path (for internal use by Java backend).
    """
    if not RAG_ENABLED:
        raise HTTPException(status_code=503, detail="RAG service is not available")
    
    print(f"=== Ingesting document by path: {request.file_path} ===")
    
    if not os.path.exists(request.file_path):
        raise HTTPException(status_code=404, detail=f"File not found: {request.file_path}")
    
    try:
        result = await run_in_threadpool(ingest_document, request.file_path, request.doc_id)
        return result
    except Exception as e:
        print(f"Ingestion error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

class DeleteRequest(BaseModel):
    doc_id: str

@app.delete("/api/v1/vectors/{doc_id}")
async def delete_vectors(doc_id: str):
    """
    Delete all vectors associated with a document.
    Called when a document is deleted from the system.
    """
    if not RAG_ENABLED:
        raise HTTPException(status_code=503, detail="RAG service is not available")
    
    print(f"=== Deleting vectors for doc_id: {doc_id} ===")
    
    try:
        result = await run_in_threadpool(delete_document_vectors, doc_id)
        return result
    except Exception as e:
        print(f"Delete error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/chat/stream")
async def stream_chat(request: ChatRequest):
    print(f"=== Received chat request ===")
    print(f"Message: {request.message}")
    print(f"History count: {len(request.history)}")
    print(f"RAG enabled: {request.use_rag and RAG_ENABLED}")

    # Build messages with history
    messages = []
    
    # RAG: Search for relevant context
    system_prompt = None
    citations = []
    
    if request.use_rag and RAG_ENABLED:
        try:
            # search_context now returns list of dicts with metadata
            context_results = await run_in_threadpool(
                search_context, 
                request.message, 
                k=4, 
                doc_id=request.doc_id
            )
            
            if context_results:
                # Extract text for prompt
                context_chunks = [item["text"] for item in context_results]
                
                # Prepare citations for frontend
                citations = []
                for item in context_results:
                    citations.append({
                        "doc_id": item.get("doc_id"),
                        "text": item.get("text")[:200] + "...", # Preview
                        "chunk_index": item.get("chunk_index"),
                        "source_file": item.get("source_file")
                    })
                
                system_prompt = build_rag_prompt(request.message, context_chunks)
                print(f"RAG: Found {len(context_results)} context chunks")
                
                # Emit citation event properly as JSON
                citation_event = {
                    "type": "citation",
                    "citations": citations
                }
                # Double newline is handled by stream_llm_response wrapper usually, 
                # but here we yield it directly before the generator starts
                # We need to make sure stream_llm_response handles this or yield it here
                
        except Exception as e:
            print(f"RAG search failed: {e}")
            import traceback
            traceback.print_exc()
            # Continue without RAG context
    
    # Add system prompt if RAG found context
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    
    # Add conversation history
    for msg in request.history:
        messages.append({"role": msg.role, "content": msg.content})
    messages.append({"role": "user", "content": request.message})

    return StreamingResponse(stream_llm_response(messages, initial_event=citation_event if citations else None), media_type="text/event-stream")

@app.post("/api/v1/analyze/summary")
async def analyze_summary(request: AnalysisRequest):
    """
    Generate a summary for a specific document.
    Uses Map-Reduce strategy for large documents.
    """
    if not RAG_ENABLED:
        raise HTTPException(status_code=503, detail="RAG service is not available")
    
    print(f"=== Generating summary for: {request.doc_id} ===")
    
    # 1. Get all text chunks
    chunks = await run_in_threadpool(get_document_chunks, request.doc_id)
    if not chunks:
        raise HTTPException(status_code=404, detail=f"No content found for document: {request.doc_id}")
    
    full_text = "\n\n".join(chunks)
    total_chars = len(full_text)
    
    print(f"Document size: {total_chars} characters, {len(chunks)} chunks")
    
    # 2. Choose strategy based on document size
    if total_chars <= 30000:
        # Small document: Direct summarization
        print("Using direct summarization (small document)")
        return StreamingResponse(
            direct_summarize(full_text), 
            media_type="text/event-stream"
        )
    else:
        # Large document: Map-Reduce summarization
        print(f"Using Map-Reduce summarization (large document: {total_chars} chars)")
        return StreamingResponse(
            map_reduce_summarize(chunks), 
            media_type="text/event-stream"
        )


def direct_summarize(text: str):
    """Direct summarization for small documents."""
    system_prompt = """‰Ω†ÊòØ‰∏Ä‰∏™‰∏ì‰∏öÁöÑÂ≠¶ÊúØÁ†îÁ©∂Âä©Êâã„ÄÇËØ∑‰ªîÁªÜÈòÖËØªÁî®Êà∑Êèê‰æõÁöÑÊñáÊ°£ÂÜÖÂÆπÔºåÂπ∂ÁîüÊàê‰∏Ä‰ªΩÈ´òË¥®ÈáèÁöÑÊëòË¶Å„ÄÇ
ÊëòË¶ÅÂ∫îÂåÖÂê´Ôºö
1. Ê†∏ÂøÉÁ†îÁ©∂ÈóÆÈ¢ò
2. ‰∏ªË¶ÅÊñπÊ≥ï
3. ÂÖ≥ÈîÆÂèëÁé∞‰∏éÁªìËÆ∫
ËØ∑‰ΩøÁî® Markdown Ê†ºÂºèÔºåÂ±ÇÁ∫ßÊ∏ÖÊô∞„ÄÇ"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"ËØ∑ÊÄªÁªì‰ª•‰∏ãÊñáÊ°£ÂÜÖÂÆπÔºö\n\n{text}"}
    ]
    
    yield from stream_llm_response(messages)


def map_reduce_summarize(chunks: List[str]):
    """
    Map-Reduce summarization for large documents.
    
    Steps:
    1. Group chunks into sections (~20k chars each)
    2. Summarize each section (Map)
    3. Combine section summaries (Reduce)
    """
    # === Step 1: Group chunks into sections ===
    SECTION_SIZE = 20000  # ÊØè‰∏™ÈÉ®ÂàÜÁ∫¶ 20k Â≠óÁ¨¶
    sections = []
    current_section = []
    current_size = 0
    
    for chunk in chunks:
        chunk_size = len(chunk)
        if current_size + chunk_size > SECTION_SIZE and current_section:
            # Start new section
            sections.append("\n\n".join(current_section))
            current_section = [chunk]
            current_size = chunk_size
        else:
            current_section.append(chunk)
            current_size += chunk_size
    
    # Add last section
    if current_section:
        sections.append("\n\n".join(current_section))
    
    print(f"Split into {len(sections)} sections for Map-Reduce")
    
    # === Step 2: Map - Summarize each section ===
    yield "data: üìä **ÂºÄÂßãÂàÜÊÆµÂ§ÑÁêÜÊñáÊ°£** (ÂÖ± {0} ‰∏™ÈÉ®ÂàÜ)...\n\n".format(len(sections))
    
    section_summaries = []
    for i, section in enumerate(sections):
        yield f"data: \n\n‚è≥ Ê≠£Âú®Â§ÑÁêÜÁ¨¨ {i+1}/{len(sections)} ÈÉ®ÂàÜ...\n\n\n\n"
        
        # Generate section summary (non-streaming for internal processing)
        summary = generate_section_summary(section, i + 1)
        section_summaries.append(summary)
        
        yield f"data: ‚úÖ ÂÆåÊàêÁ¨¨ {i+1} ÈÉ®ÂàÜ\n\n\n\n"
    
    # === Step 3: Reduce - Combine section summaries ===
    yield "data: \n\nüîÑ **Ê±áÊÄªÊâÄÊúâÈÉ®ÂàÜ**...\n\n\n\n"
    
    combined_summaries = "\n\n".join([
        f"„ÄêÁ¨¨ {i+1} ÈÉ®ÂàÜÊëòË¶Å„Äë\n{summary}" 
        for i, summary in enumerate(section_summaries)
    ])
    
    # Final synthesis
    final_prompt = f"""‰Ω†ÊòØ‰∏Ä‰∏™Â≠¶ÊúØÁ†îÁ©∂‰∏ìÂÆ∂„ÄÇÊàëÂ∑≤ÁªèÂ∞Ü‰∏Ä‰ªΩÈïøÊñáÊ°£ÂàÜÊàê‰∫Ü {len(sections)} ‰∏™ÈÉ®ÂàÜÔºåÂπ∂ÂØπÊØè‰∏™ÈÉ®ÂàÜÁîüÊàê‰∫ÜÊëòË¶Å„ÄÇ
Áé∞Âú®ËØ∑‰Ω†Â∞ÜËøô‰∫õÈÉ®ÂàÜÊëòË¶ÅÊï¥ÂêàÊàê‰∏Ä‰ªΩËøûË¥Ø„ÄÅÂÆåÊï¥ÁöÑÊúÄÁªàÊëòË¶Å„ÄÇ

Ë¶ÅÊ±ÇÔºö
1. ‰øùÁïôÊâÄÊúâÂÖ≥ÈîÆ‰ø°ÊÅØ
2. ÂéªÈô§ÂÜó‰ΩôÂÜÖÂÆπ
3. Á°Æ‰øùÈÄªËæëËøûË¥Ø
4. ‰ΩøÁî® Markdown Ê†ºÂºè

‰ª•‰∏ãÊòØÂêÑÈÉ®ÂàÜÊëòË¶ÅÔºö

{combined_summaries}

ËØ∑ÁîüÊàêÊúÄÁªàÊëòË¶ÅÔºö"""
    
    messages = [
        {"role": "user", "content": final_prompt}
    ]
    
    yield "data: \n\n---\n\n## üìù ÊúÄÁªàÊëòË¶Å\n\n\n\n"
    yield from stream_llm_response(messages)


def generate_section_summary(section_text: str, section_num: int) -> str:
    """
    Generate a summary for a single section (synchronous).
    Returns the summary text.
    """
    system_prompt = f"""‰Ω†ÊòØ‰∏Ä‰∏™Â≠¶ÊúØÁ†îÁ©∂Âä©Êâã„ÄÇËøôÊòØ‰∏Ä‰ªΩÈïøÊñáÊ°£ÁöÑÁ¨¨ {section_num} ÈÉ®ÂàÜ„ÄÇ
ËØ∑ÁîüÊàê‰∏Ä‰ªΩÁÆÄÊ¥ÅÁöÑÊëòË¶ÅÔºåÂåÖÂê´ËøôÈÉ®ÂàÜÁöÑÂÖ≥ÈîÆ‰ø°ÊÅØ„ÄÇ
ÊëòË¶ÅÂ∫îÁÆÄÊ¥Å‰ΩÜÂÆåÊï¥ÔºåÁ∫¶ 200-300 Â≠ó„ÄÇ"""
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": section_text[:15000]}  # Limit to prevent overflow
    ]
    
    # Call API synchronously (non-streaming)
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json",
    }
    data = {
        "model": MODEL_NAME,
        "messages": messages,
        "stream": False  # Non-streaming for internal use
    }
    
    try:
        response = requests.post(BASE_URL, headers=headers, json=data, timeout=60)
        if response.status_code == 200:
            result = response.json()
            return result["choices"][0]["message"]["content"]
        else:
            return f"[ÊëòË¶ÅÁîüÊàêÂ§±Ë¥•: HTTP {response.status_code}]"
    except Exception as e:
        print(f"Section summary error: {e}")
        return f"[ÊëòË¶ÅÁîüÊàêÂá∫Èîô: {str(e)}]"


@app.post("/api/v1/analyze/comparison")
async def analyze_comparison(request: ComparisonRequest):
    """
    Compare multiple documents with structured comparison table.
    """
    if not RAG_ENABLED:
        raise HTTPException(status_code=503, detail="RAG service is not available")
    
    print(f"=== Comparing {len(request.doc_ids)} documents ===")
    
    # 1. Retrieve document contents
    doc_contents = []
    doc_titles = []
    for doc_id in request.doc_ids:
        chunks = await run_in_threadpool(get_document_chunks, doc_id)
        if chunks:
            text = "\n\n".join(chunks)[:15000]  # Limit each doc
            doc_contents.append(text)
            doc_titles.append(str(doc_id))
    
    if len(doc_contents) < 2:
        raise HTTPException(status_code=400, detail="Need at least 2 documents for comparison")
    
    # 2. Generate structured comparison
    return StreamingResponse(
        generate_structured_comparison(doc_contents, doc_titles, request.aspects),
        media_type="text/event-stream"
    )


def generate_structured_comparison(doc_contents: List[str], doc_titles: List[str], custom_aspects: Optional[List[str]] = None):
    """
    Generate structured comparison with table data and detailed analysis.
    
    Steps:
    1. Generate comparison table (JSON)
    2. Yield table as SSE event
    3. Generate detailed analysis (streaming)
    """
    # === Step 1: Define comparison dimensions ===
    if custom_aspects and len(custom_aspects) > 0:
        dimensions = custom_aspects
    else:
        dimensions = ["Á†îÁ©∂ÈóÆÈ¢ò/ÁõÆÊ†á", "Á†îÁ©∂ÊñπÊ≥ï", "‰∏ªË¶ÅÂèëÁé∞", "ÂàõÊñ∞ÁÇπ", "Â±ÄÈôêÊÄß"]
    
    # === Step 2: Generate comparison table ===
    yield "data: üìä Ê≠£Âú®ÁîüÊàêÂØπÊØîË°®Ê†º...\n\n"
    
    # Build prompt for table generation
    doc_contents_combined = ""
    for i, (title, content) in enumerate(zip(doc_titles, doc_contents)):
        doc_contents_combined += f"\n\n„ÄêÊñáÊ°£ {i+1}: {title}„Äë\n{content}\n"
    
    table_prompt = f"""‰Ω†ÊòØ‰∏Ä‰∏™Â≠¶ÊúØÂØπÊØîÂàÜÊûê‰∏ìÂÆ∂„ÄÇËØ∑ÂØπ‰ª•‰∏ã {len(doc_contents)} ÁØáÊñáÊ°£ËøõË°åÁªìÊûÑÂåñÂØπÊØîÂàÜÊûê„ÄÇ

ÂØπÊØîÁª¥Â∫¶Ôºö{', '.join(dimensions)}

ÊñáÊ°£ÂÜÖÂÆπÔºö
{doc_contents_combined}

ËØ∑‰∏•Ê†ºÊåâÁÖß‰ª•‰∏ã JSON Ê†ºÂºèËæìÂá∫ÂØπÊØîË°®Ê†ºÊï∞ÊçÆÔºå‰∏çË¶ÅÊ∑ªÂä†‰ªª‰ΩïÂÖ∂‰ªñÊñáÂ≠óÔºö

{{
  "dimensions": {dimensions},
  "comparison": [
    ["{dimensions[0]}ÁöÑÊñáÊ°£1ÂÜÖÂÆπ", "{dimensions[0]}ÁöÑÊñáÊ°£2ÂÜÖÂÆπ", ...],
    ["{dimensions[1]}ÁöÑÊñáÊ°£1ÂÜÖÂÆπ", "{dimensions[1]}ÁöÑÊñáÊ°£2ÂÜÖÂÆπ", ...],
    ...
  ]
}}

Ë¶ÅÊ±ÇÔºö
1. ÊØè‰∏™Áª¥Â∫¶ÁöÑÂÜÖÂÆπË¶ÅÁÆÄÊ¥ÅÔºà50-100Â≠óÔºâ
2. Á™ÅÂá∫ÂÖ≥ÈîÆÂ∑ÆÂºÇ
3. ‰ΩøÁî®‰∏ì‰∏öÊúØËØ≠"""

    # Call LLM to generate table (non-streaming for parsing)
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json",
    }
    data = {
        "model": MODEL_NAME,
        "messages": [{"role": "user", "content": table_prompt}],
        "stream": False
    }
    
    try:
        response = requests.post(BASE_URL, headers=headers, json=data, timeout=60)
        if response.status_code == 200:
            result = response.json()
            table_text = result["choices"][0]["message"]["content"]
            
            # Parse JSON from response
            import re
            json_match = re.search(r'\{[\s\S]*\}', table_text)
            if json_match:
                table_json = json_match.group(0)
                # Emit table event
                table_event = {
                    "type": "comparison_table",
                    "documents": [{"id": title, "title": f"ÊñáÊ°£{i+1}"} for i, title in enumerate(doc_titles)],
                    "table_data": table_json  # String JSON to be parsed by frontend
                }
                yield f"data: {json.dumps(table_event, ensure_ascii=False)}\n\n"
            else:
                yield "data: ‚ö†Ô∏è Ë°®Ê†ºÁîüÊàêÂ§±Ë¥•ÔºåÂ∞ÜÁõ¥Êé•ÊòæÁ§∫ËØ¶ÁªÜÂàÜÊûê\n\n"
        else:
            yield "data: ‚ö†Ô∏è Ë°®Ê†ºÁîüÊàêÂ§±Ë¥•ÔºåÂ∞ÜÁõ¥Êé•ÊòæÁ§∫ËØ¶ÁªÜÂàÜÊûê\n\n"
    except Exception as e:
        print(f"Table generation error: {e}")
        yield "data: ‚ö†Ô∏è Ë°®Ê†ºÁîüÊàêÂ§±Ë¥•ÔºåÂ∞ÜÁõ¥Êé•ÊòæÁ§∫ËØ¶ÁªÜÂàÜÊûê\n\n"
    
    # === Step 3: Generate detailed analysis ===
    yield "data: \n\n---\n\n## üìù ËØ¶ÁªÜÂØπÊØîÂàÜÊûê\n\n\n\n"
    
    analysis_prompt = f"""‰Ω†ÊòØ‰∏Ä‰∏™‰∏ì‰∏öÁöÑÂ≠¶ÊúØÊÉÖÊä•ÂàÜÊûêÂ∏à„ÄÇÂü∫‰∫é‰ª•‰∏ãÊñáÊ°£ÔºåÁîüÊàê‰∏Ä‰ªΩÊ∑±Â∫¶ÂØπÊØîÂàÜÊûêÊä•Âëä„ÄÇ

ÊñáÊ°£ÂÜÖÂÆπÔºö
{doc_contents_combined}

ËØ∑‰ªé‰ª•‰∏ãËßíÂ∫¶ËøõË°åËØ¶ÁªÜÂØπÊØîÂàÜÊûêÔºö
1. Á†îÁ©∂ËÉåÊôØ‰∏éÂä®Êú∫ÁöÑÂºÇÂêå
2. ÊñπÊ≥ïËÆ∫ÁöÑÂ∑ÆÂºÇ‰∏é‰ºòÂä£
3. Ê†∏ÂøÉÂèëÁé∞ÁöÑ‰∫íË°•ÊÄßÊàñÂÜ≤Á™Å
4. ÂàõÊñ∞ÁÇπÁöÑÊØîËæÉ
5. Â∫îÁî®ÂâçÊôØ‰∏éÂ±ÄÈôêÊÄß

**Ê†ºÂºèË¶ÅÊ±Ç**Ôºö
- ‰ΩøÁî® Markdown Ê†ºÂºè
- ‰ΩøÁî®Ê†áÈ¢ò„ÄÅÂàóË°®„ÄÅÂä†Á≤óÁ≠âÊ†ºÂºèÁªÑÁªáÂÜÖÂÆπ
- **Á¶ÅÊ≠¢‰ΩøÁî® Markdown Ë°®Ê†º**ÔºàÂØπÊØîË°®Ê†ºÂ∑≤Âú®‰∏äÊñπÂçïÁã¨Â±ïÁ§∫Ôºâ
- Â±ÇÊ¨°Ê∏ÖÊô∞ÔºåÂÜÖÂÆπËØ¶ÂÆû"""
    
    messages = [{"role": "user", "content": analysis_prompt}]
    yield from stream_llm_response(messages)


@app.post("/api/v1/write/process")
async def write_process(request: WriteRequest):
    """
    Process text for writing assistance (polish, expand, etc).
    """
    instruction_map = {
        "polish": "ËØ∑Ê∂¶Ëâ≤‰ª•‰∏ãÊñáÊú¨Ôºå‰ΩøÂÖ∂Êõ¥Âä†Â≠¶ÊúØÂåñ„ÄÅÊ≠£Âºè‰∏îÊµÅÁïÖÔºå‰øÆÊ≠£ËØ≠Ê≥ïÈîôËØØÔºå‰ΩÜ‰øùÊåÅÂéüÊÑèÔºö",
        "expand": "ËØ∑ÂØπ‰ª•‰∏ãËßÇÁÇπËøõË°åÊâ©ÂÜôÔºåË°•ÂÖÖÊõ¥Â§öÁªÜËäÇ„ÄÅËÆ∫ÊçÆÊàñËÉåÊôØ‰ø°ÊÅØÔºå‰ΩøÂÖ∂ÂÜÖÂÆπÊõ¥ÂÖÖÂÆûÔºö",
        "continue": "ËØ∑Ê†πÊçÆ‰ª•‰∏ã‰∏äÊñáÔºåÁª≠ÂÜô‰∏ÄÊÆµÈÄªËæëËøûË¥ØÁöÑÂÜÖÂÆπÔºö",
        "fix_grammar": "ËØ∑Ê£ÄÊü•Âπ∂‰øÆÊ≠£‰ª•‰∏ãÊñáÊú¨ÁöÑËØ≠Ê≥ïÂíåÊãºÂÜôÈîôËØØÔºåËæìÂá∫‰øÆÊ≠£ÂêéÁöÑÁâàÊú¨Ôºö"
    }
    
    specific_instruction = instruction_map.get(request.instruction, "ËØ∑Â§ÑÁêÜ‰ª•‰∏ãÊñáÊú¨Ôºö")
    
    system_prompt = """‰Ω†ÊòØ‰∏Ä‰∏™ËµÑÊ∑±ÁöÑÂ≠¶ÊúØÂÜô‰ΩúÂØºÂ∏à„ÄÇ
    „ÄêÈáçË¶ÅËßÑÂàô„Äë
    1. ‰Ω†ÂøÖÈ°ªÁõ¥Êé•ËæìÂá∫Â§ÑÁêÜÂêéÁöÑÊñáÊú¨ÂÜÖÂÆπ„ÄÇ
    2. ‰∏•Á¶ÅÂåÖÂê´‰ªª‰ΩïËß£Èáä„ÄÅÂâçË®Ä„ÄÅÂêéÁºÄ„ÄÅÁî±‰∫é„ÄÅÊîπÂÜôËØ¥ÊòéÁ≠âÂÖÉÊï∞ÊçÆ„ÄÇ
    3. Âç≥‰ΩøÊñáÊú¨ÂæàÁü≠Ôºå‰πüÂè™ËæìÂá∫ÁªìÊûú„ÄÇ
    4. „ÄêÊ†ºÂºèË¶ÅÊ±Ç„ÄëÔºö
       - **ÂèØ‰ª•‰ΩøÁî®** Â∫èÂè∑Ê†áÈ¢òÊù•ÁªÑÁªáÁªìÊûÑÔºà‰æãÂ¶ÇÔºö"1. Á†îÁ©∂ËÉåÊôØ" Êàñ "‰∏Ä„ÄÅ ÊñπÊ≥ïÊèèËø∞"ÔºâÔºåËøôÂ∞Ü‰æø‰∫éÂêéÁª≠ÁîüÊàê Word ÁõÆÂΩï„ÄÇ
       - **‰∏çË¶Å‰ΩøÁî®** Markdown ÁöÑ # Á¨¶Âè∑‰Ωú‰∏∫Ê†áÈ¢ò„ÄÇ
       - **‰∏çË¶Å‰ΩøÁî®** **Âä†Á≤ó** Êàñ *Êñú‰Ωì* Á¨¶Âè∑Ôºà‰øùÊåÅÁ∫ØÊñáÊú¨Êï¥Ê¥ÅÔºâ„ÄÇ
       - ‰ªÖ‰ΩøÁî®Á∫ØÊñáÊú¨ÊÆµËêΩÔºåÊÆµËêΩ‰πãÈó¥Áî®Á©∫Ë°åÂàÜÈöî„ÄÇ
    """
    
    # Wrap specific instruction to reinforce the rule
    final_instruction = f"{specific_instruction}\n(ËØ∑Áõ¥Êé•ËæìÂá∫ÁªìÊûúÔºåÂèØ‰ΩøÁî®'1.'Êàñ'‰∏Ä„ÄÅ'‰Ωú‰∏∫Â±ÇÁ∫ßÊ†áÈ¢òÔºå‰∏çË¶ÅÂåÖÂê´ÂÖ∂‰ªñËß£Èáä)"

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"{final_instruction}\n\n„ÄêÁî®Êà∑ÊñáÊú¨„Äë\n{request.text}"}
    ]
    
    # Include user's custom context/requirements if provided
    if request.context and request.context.strip():
        messages[1]["content"] += f"\n\n„ÄêÈ¢ùÂ§ñË¶ÅÊ±Ç„Äë\n{request.context}"
    
    return StreamingResponse(stream_llm_response(messages), media_type="text/event-stream")

# Helper to avoid code duplication
def stream_llm_response(messages, initial_event=None):
    # Send initial event if provided (e.g., citations)
    if initial_event:
        yield f"data: {json.dumps(initial_event)}\n\n"
        
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json",
        "HTTP-Referer": "https://localhost:5173",
        "X-Title": "LLM Research Assistant"
    }
    data = {
        "model": MODEL_NAME,
        "messages": messages,
        "stream": True
    }
    try:
        # Timeout: (Connect, Read)
        response = requests.post(BASE_URL, headers=headers, json=data, stream=True, timeout=(10, 180))
        if response.status_code != 200:
             yield f"data: Error: API returned {response.status_code}\n\n"
             yield "data: [DONE]\n\n"
             return

        for line in response.iter_lines():
            if line:
                decoded = line.decode('utf-8')
                if decoded.startswith("data: "):
                    data_str = decoded[6:]
                    
                    if data_str.strip() == "[DONE]":
                        yield "data: [DONE]\n\n"
                        break
                    
                    try:
                        data_json = json.loads(data_str)
                        if "choices" in data_json and len(data_json["choices"]) > 0:
                            delta = data_json["choices"][0].get("delta", {})
                            content = delta.get("content", "")
                            if content:
                                # Send plain text, escape newlines for SSE
                                escaped = content.replace('\n', '\\n')
                                yield f"data: {escaped}\n\n"
                    except json.JSONDecodeError:
                        pass
    except Exception as e:
        yield f"data: Error: {str(e)}\n\n"
        yield "data: [DONE]\n\n"

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
